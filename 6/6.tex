\documentclass{article}
\usepackage[margin=1in]{geometry}
\usepackage{../common}
\usepackage{../pagesetup}



\begin{document}
\lecture{6}{September 20}{Sasha Rush}{Meena Jagadeesan, Yufeng Ling, Tomoka Kan, Wenting Cai, Elana Simon}{Exponential Families}

\subsection{Introduction}
(Wainwright and Jordan (textbook) presents a more detailed coverage of the material in this lecture.)

\medskip

\noindent This lecture, we will unify all of the fundamentals presented so far:
\begin{center}
\begin{tabular}{c | c | c }
    $p(\theta)$ & $p(x)$ & $p(y \mid x) / p(x, y)$  \\ \hline
    Beta, Dir & Discrete & Classification \\ \hline
    MVN, IW & MVN & Linear Regression \\ \hline
    & \textbf{Exponential Families} & \textbf{Generalized Linear Models} \\ \hline
     &Undirected Graphic Models & Conditional UGM \\ \hline
    & Variational Inference %move this to the middle
\end{tabular}
\end{center}
We will focus on coming up with a general form for Discrete and MVN through exponential families. We will also come up with a general form for classification and linear regression through generalized linear models. 

\subsection{Definition of Exponential Family}
The definition is
\begin{align*}
p(x \mid \theta(\mu)) &= \frac{1}{Z(\theta)} h(x) \exp\{\theta^T \phi(x)\} \\
&= h(x) \exp{\theta^T \phi(x) - A(\theta)}
\end{align*}
where
\medskip

\begin{tabular}{r | l}
    $\mu$ & mean parameters \\ \hline
    $\theta(\mu)$ & natural / canonical / exponential parameters \\ \hline
    $Z(\theta)A(\theta)$ & also written as $Z(\theta(\mu))$ or $Z(\mu)$, the partition function and log partition \\ \hline
    $\phi(x)$ & sufficient statistics of $x$, potential functions, ``features'' \\ \hline
    $h(x)$ & scaling term, in most cases, we have $h(x) = 1$
\end{tabular}
\medskip

\noindent Note that there is ``minimal form'' and ``overcomplete form''.

\subsection{Examples of Exponential Families}

\subsubsection{Bernoulli/Categorical}
First, we consider the Bernoulli as an exponential family. Like last lecture, we rewrite the distribution as an $\exp$ of $\log$.

\begin{align*}    
\text{Ber}(x | \mu) &= \mu^x(1-\mu)^{(1-x)} \\
 &= \exp{x\log \mu + (1-x)\log(1-\mu)} \\
 &= \underbrace{}_{h(x)}\exp{ \underbrace{\log\left(\frac{\mu }{1- \mu}\right)}_{\theta} \underbrace{x}_{\phi(x)} + \underbrace{\log (1- \mu)}_{-A(\mu)}}
\end{align*}

For the \textbf{minimal form}, we have 

\begin{align*}
    h(x) &= 1 \\
    \phi_1(x) &= x \\
    \theta_1(\mu) &= \log\frac{\mu}{1 - \mu} (\text{``log odds''}) \\
    \mu &= \sigma(\theta)\\
    A(\mu) &= -\log(1-\mu)\\
    A(\theta) &= -\log(1-\sigma(\theta)) = \theta + \log(1+ e^{-\theta})
\end{align*}

For the \textbf{overcomplete form}, we have 
\begin{align*}
    \phi(x) &= \begin{bmatrix} x \\ 1 - x \end{bmatrix}\\
    \theta &= \begin{bmatrix} \log \mu \\ \log (1- \mu) \end{bmatrix}
\end{align*}

For the Categorical/Multinouilli distribution, we have
$$\theta = \begin{bmatrix}
\log \mu_1 \\
\vdots \\
\log \mu_n
\end{bmatrix}
$$
where $\sum_{c} \mu_c = 1$.

\medskip

In the minimal form, the components of the sufficient statistics are linearly independent and the natural parameters are uniquely identifiable. In the over-complete form, there is a linear dependence between the features so the natural parameters not uniquely identifiable.

\medskip

Note: Writing out in overcomplete form usually comes with some restraints.

\subsubsection{Univariate Gaussians}

\begin{align*}
\mathcal{N} ( x \mid \mu, \sigma^2) &= (2 \pi \sigma^2)^{1/2} \exp \{ -\frac{1}{2\sigma^2} (x - \mu)^2 \} \\
&= \underbrace{(2\pi \sigma^2)^{-\frac{1}{2}}}_{A(\mu, \sigma^2)} \exp\{\underbrace{-\frac{1}{2\sigma^2}x^2 + \frac{\mu}{\sigma^2}x}_{\theta^T \phi(x)} - \underbrace{\frac{1}{2\sigma^2} \mu^2}_{A(\mu, \theta^2)}\}
\end{align*}

\begin{align*}
\phi(x) &= \begin{bmatrix} x \\ x^2 \end{bmatrix} \\
\theta &= \begin{bmatrix} \frac{\mu}{\sigma^2} \\ -\frac{1}{2\sigma^2} \end{bmatrix}\\
A(\mu, \sigma^2) &= \frac{1}{2}\log(2\pi\sigma^2) + \frac{1}{2\sigma^2}\mu^2
\\
\mu &= - \frac{\theta_1}{2\theta_2}
\\
\sigma^2 &= - \frac{1}{2\theta_2}
\\
A(\theta) &= - \frac12 \log( -2 \theta_2) - \frac{\theta_1^2}{4 \theta_2}
\end{align*}

\subsubsection{Bad distributions}
Two simple distributions that do not fit this form are the uniform distribution and the Student's t-distribution.

\medskip


In an exponential family, $h(x)$ is the scaling constant that captures the support of the distribution. We see that $h$ can only depend on x, and thus the support of an exponential family must remain the same regardless of the parameters of the distribution.

For example, the parameters $a,b$ of a uniform distribution $\trm{Unif}(a,b)$ are precisely what defines the support of the distribution, so it could not be encoded as an exponential family. However, if a and b were left as constants (as in the standard normal), then the scale would not depend on any parameters, but then we also wouldn't be looking at a \textit{family} of distributions, just a single distribution.

\subsection{Properties of Exponential Families}
Most inference problems involve a mapping between natural parameters and mean parameters, so this is a natural framework. 


Here are three properties of exponential families:

\paragraph{Property 1} Derivatives of $A(\theta)$ provide us the cumulants of the distribution $\mathbb{E}(\phi(x))$, $\text{var}(\phi(x))$:
\begin{proof}
For univariate (partially taken from Murphy):
\begin{align*}
\frac{d A}{d \theta} &= \frac{d}{d\theta}(\log Z(\theta)) \\
&= \frac{d}{d \theta} \log \underbrace{\left(\int \exp\{\theta \phi\} h(x) dx\right)}_{\text{needed to integrate to $1$}} \\
&= \frac{\int\phi \exp\{\theta\phi\}h(x) dx}{\int \exp( \theta \phi) h(z) dx}\\
&= \frac{\int\phi \exp\{\theta\phi\}h(x) dx}{\exp(A(\theta))} \\
&= \int \phi(x) \underbrace{\exp(\theta \phi(x) - A(\theta)) h(x)}_{p(x)} dx\\
&= \int \phi(x) p(x) dx \\
&= \mathbb{E}(\phi(x))\\
\frac{d^2A}{d\theta^2} &= \int \phi(x)\exp(\theta\phi(x) - A(\theta))h(x)(h(x)(\phi(x) - A'(\theta))dx \\
&= \int \phi(x)p(x)(\phi(x) - A'(\theta))dx\\
&= \int \phi^2(x)p(x)dx - \underbrace{A'(\theta)}_{\frac{dA}{d\theta}=\mathbb{E}(\phi(x))} \int \phi(x)p(x)dx \\
&= \mathbb{E}(\phi^2(X)) - \mathbb{E}(\phi(x))^2 \\
&= \text{var}(\phi(x))
\end{align*}
The same property holds for multivariates. In this case we have:
\begin{align*}
\frac{\partial^2A}{\partial \theta_i \partial\theta_j} &= \mathbb{E}(\phi_i(x)\phi_j(x)) - \mathbb{E}(\phi_i(x))\mathbb{E}(\phi_j(x))\\
\nabla^2 A(\theta) &= \text{cov}(\phi(x))
\end{align*}

\end{proof}

\textbf{Bernoulli}: 
\[A(\theta) = \theta + \log(1 + e^{-\theta})\]
\[\frac{d A}{d\theta} = 1 - \frac{e^{-\theta}}{1 + e^{-\theta}} = \underbrace{\frac{1}{1 + e^{-\theta}}}_{\text{sigmoid}} = \sigma(\theta) = \mu \]

\textbf{Univariate Normal} Left as exercise.
\paragraph{Property 2} MLE has a nice form (through ``moment matching'')
\begin{proof}
\begin{align*} \argmax{\theta} \log p(\text{data} \mid \theta) &= \argmax{\theta} \left(\sum_d \theta^T\phi(x_d)\right) - NA(\theta) \\
&= \argmax{\theta}\theta^T\underbrace{\left(\sum_d\phi(x_d)\right)}_{\text{sum of sufficient statistics}} - \underbrace{NA(\theta)}_{\text{amount of points}}
\end{align*}

We take a derivative to obtain:
\begin{align*}
\frac{d(.)}{d \theta} &= \sum_d \phi(x_d) - N \frac{dA(\theta)}{d \theta} \\
&= \sum_d\phi(x_d) - N\mathbb{E}(\phi(x)) \\
 &= 0
\end{align*}
\[{E}(\phi(x)) = \underbrace{\frac{\sum\phi(x_d)}{N}}_{\text{set mean parameter to sample means that gives us MLE}}
\]
\end{proof}

\paragraph{Property 3} Exponential families have conjugate priors.

\begin{proof}
We first introduce some notations. 
\begin{align*}
\eta &\text{ - parameters}
\\
\bar{s} &= \sum_d\phi(x_d) / N
\\
p(\text{data} \mid \eta) &\propto \exp [(N \bar s) \eta - N A(\eta)]
\\
p(\eta \mid N_0, s_0) &\propto \exp[ (N_0, \bar s_0) \eta - N_0 \underbrace{A(\eta)}_{\text{not log partition, which has to be a function strictly of parameters}}]
\\
p(\eta | \text{data}) &\propto \exp((N\bar{s} + N_0\bar{s}_0)^T\eta - (N_0 + N)A(\eta))
\end{align*}
The above two distributions have the same sufficient statistics -- so we have a conjugate prior. It also tells us that it is not a coincidence that we kept obtaining pseudo counts. (More references will be put up to describe this). 

\end{proof}

\subsection{Definition of Generalized Linear Models}

While exponential families generalize $p(x)$, GLMs generalize $p(y|x)$.

\[
p(y | x,w) = h(y)\exp\{\theta(\underbrace{\mu(x)}_{\text{predict mean}})^T\phi(y) - A(\theta)\}
\]
where $\mu(x) = \underbrace{g^{-1}}_{\text{squashing const}}(w^Tx + b)$
where $g$ is an appropriate linear transformation.

This can be summarized through the following sequence of transformations:
$$
x \overset{g^{-1}(w^T x + b)}{\longrightarrow} \mu \to \theta \to p(y \mid x).
$$

\subsection{Examples of Generalized Linear Models}
We present three examples:
\paragraph{Example 1} Exponential family - Normal distribution with $\sigma^2 = 1$ and $g^{-1}$ is the identity function.
This gives us the linear regression
$$
\mu = w^Tx + b \qquad \mathbb{R} \to \mathbb{R}.
$$

\paragraph{Example 2} Exponential family - Bernoulli distribution and $g^{-1}$ is the sigmoid function $\sigma: \mathbb{R} \rightarrow (0, 1)$.
Now, $\mu = \sigma(w^Tx + b)$ and $\theta = \log \left(\frac{\mu }{1 - \mu}\right)$. This is how we define logistic regression. This gives us

$$p(y \mid x) = \sigma(w^Tx + b)^y(1 - \sigma(w^Tx + b))^{1-y}$$

\paragraph{Example 3} Exponential family - Categorical distribution with $g^{-1}$ as the softmax function.

$\mu_c = \text{softmax}(w_c^T x + b_c)_c$

$\theta_c = \log \mu_c$
\end{document}
